# AI Feedback and Assessment

## Guiding Premise

AI inside A²I assessments exists to **increase coherence**, not to surveil or shortcut learning. Faculty and learners co-design how intelligent tools surface evidence, reflection, and risk signals in keeping with the program’s governance-as-innovation stance.【F:docs/context/conversation.md†L39-L47】【F:docs/context/conversation.md†L139-L152】

## Assessment Architecture

| Component | Weight | Role of AI |
|-----------|--------|------------|
| Applied Projects | 50% | Agents compare artifacts against mission outcomes, stakeholder promises, and governance standards before human review.【F:docs/context/conversation.md†L82-L88】 |
| Auto-Checks | 20% | Coursera/Canvas quizzes and labs provide immediate feedback, tuned to highlight coherence gaps rather than rote correctness.【F:docs/context/conversation.md†L76-L88】 |
| Oral Vivas | 20% | Speech-to-text models capture Week 8 dialogues; analytics surface themes and unanswered risks for faculty adjudication.【F:docs/context/conversation.md†L76-L88】【F:docs/context/conversation.md†L139-L152】 |
| Peer Critique | 10% | AI summarizes cross-team reviews to spotlight divergent perspectives and ensure inclusion of quieter voices.【F:docs/context/conversation.md†L144-L152】【F:docs/context/conversation.md†L156-L163】 |

## Workflow Across the 8-Week Template

1. **Week 0 – AI Use Charter:** Students draft AI-use declarations, clarifying tooling, disclosure norms, and accountability rituals. Canvas modules include exemplars and a self-audit agent that checks for clarity and completeness.【F:docs/context/conversation.md†L76-L88】
2. **Weeks 1-4 – Formative Analytics:** Auto-checks benchmark understanding of strategy, infrastructure, and causality. Feedback bots flag when evidence chains or governance controls are underspecified and route learners to relevant Coursera micro-lessons.【F:docs/context/conversation.md†L94-L114】
3. **Weeks 5-7 – Studio Evaluations:** GCP-hosted evaluation harnesses capture telemetry from prototypes. Agents compute fairness, reliability, and observability metrics, handing faculty a structured brief before live critique.【F:docs/context/conversation.md†L101-L109】【F:docs/context/conversation.md†L120-L135】
4. **Week 8 – Viva Intelligence Packets:** AI aggregates project artifacts, peer reviews, and lab telemetry into a briefing deck to prime viva panels. Humans make final determinations, while agents log improvement prompts for post-viva iteration.【F:docs/context/conversation.md†L76-L88】【F:docs/context/conversation.md†L139-L154】

## Safeguards & Governance

- **Human-in-the-Loop:** All summative judgments stay with faculty committees. AI agents provide evidence synthesis, not grades.【F:docs/context/conversation.md†L139-L152】
- **Bias & Equity Audits:** Monthly reviews of feedback models ensure Growth for Good, Finance for Inclusion, and Healthcare for Humanity learners receive equitable coaching and challenge.【F:docs/context/conversation.md†L111-L114】【F:docs/context/conversation.md†L156-L163】
- **Transparency:** Learners can inspect prompts, rubrics, and agent logs via GitHub, reinforcing open governance expectations.【F:docs/context/conversation.md†L167-L193】

## Faculty Enablement

Faculty development sessions model how to read AI-generated coherence briefs, how to author reflexive prompts, and how to adjust thresholds as new risks emerge. Mentors log refinements to rubrics and agent behavior through open PRs so the assessment system evolves alongside the program vision.【F:docs/context/conversation.md†L167-L193】【F:docs/context/conversation.md†L214-L230】
